# CLOUD-BIGDATA

## Description
This repository contains projects and exercises developed for the elective course **Cloud and Big Data**. The course focuses on working with large-scale data processing frameworks and cloud computing technologies.

The primary tools and frameworks explored in this repository include:
- **Batch Processing**:
  - **MapReduce**: A programming model used for processing and generating large datasets in parallel.
  - **Apache Spark**: A fast and general-purpose cluster-computing framework for big data processing.
- **Google Cloud**:
  - **Google Cloud Storage**: For storing and managing large datasets.
  - **Dataproc**: For running Hadoop and Spark clusters on Google Cloud.

---

## Learning Objectives
- Understand the principles of distributed data processing.
- Implement **MapReduce** jobs for processing large datasets.
- Explore **Apache Spark** for faster and more efficient batch processing.
- Deploy big data solutions on **Google Cloud Platform** (GCP).
- Learn to manage and optimize cloud resources for scalable big data processing.

---

## Tools and Technologies
The repository uses the following tools and technologies:
- **Hadoop**: For implementing and running MapReduce jobs.
- **Apache Spark**: For distributed data processing and optimization.
- **Google Cloud Platform (GCP)**:
  - **Google Cloud Storage**: For storing input and output data.
  - **Google Dataproc**: For managing Hadoop and Spark clusters in the cloud.
- **Python**: For Spark programming using PySpark.

---

## Contents
The repository includes:
1. **MapReduce**:
   - Implementation of basic MapReduce jobs.
   - Applications for word count, sorting, and log analysis.

2. **Apache Spark**:
   - Hands-on examples using RDDs (Resilient Distributed Datasets) and DataFrames.
   - Batch processing tasks for aggregating and transforming large datasets.

3. **Google Cloud**:
   - Configuration and deployment of Dataproc clusters.
   - Integration of Spark and MapReduce with Google Cloud Storage.

---
